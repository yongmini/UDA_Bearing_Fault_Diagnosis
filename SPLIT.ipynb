{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def data_split(self):\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets.SequenceDatasets import dataset\n",
    "from datasets.sequence_aug import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Digital data was collected at 12,000 samples per second\n",
    "signal_size = 1024\n",
    "dataname= {0:[\"97.mat\",\"105.mat\", \"118.mat\", \"130.mat\", \"169.mat\", \"185.mat\", \"197.mat\", \"209.mat\", \"222.mat\",\"234.mat\"],  # 1797rpm\n",
    "           1:[\"98.mat\",\"106.mat\", \"119.mat\", \"131.mat\", \"170.mat\", \"186.mat\", \"198.mat\", \"210.mat\", \"223.mat\",\"235.mat\"],  # 1772rpm\n",
    "           2:[\"99.mat\",\"107.mat\", \"120.mat\", \"132.mat\", \"171.mat\", \"187.mat\", \"199.mat\", \"211.mat\", \"224.mat\",\"236.mat\"],  # 1750rpm\n",
    "           3:[\"100.mat\",\"108.mat\", \"121.mat\",\"133.mat\", \"172.mat\", \"188.mat\", \"200.mat\", \"212.mat\", \"225.mat\",\"237.mat\"]}  # 1730rpm\n",
    "#12kdrive       noraml     ir007        b007       or007(6)        ir014      b014          or014(6)        ir021        b021      or021(6)\n",
    " \n",
    "label_mapping = {\n",
    "    0: 0,  # normal\n",
    "    1: 1, 2: 2, 3: 3,  \n",
    "    4: 1, 5: 2, 6: 3,  \n",
    "    7: 1, 8: 2, 9: 3   \n",
    "}\n",
    "datasetname = [\"12k_Drive_End_Bearing_Fault_Data\", \"12k_Fan_End_Bearing_Fault_Data\", \"48k_Drive_End_Bearing_Fault_Data\",\n",
    "               \"Normal_Baseline_Data\"]\n",
    "\n",
    "axis = [\"_DE_time\", \"_FE_time\", \"_BA_time\"]\n",
    "\n",
    "label = [i for i in range(0, 10)]\n",
    "\n",
    "def get_files(root, N,source=True):\n",
    "    '''\n",
    "    This function is used to generate the final training set and test set.\n",
    "    root:The location of the data set\n",
    "    '''\n",
    "    data = []\n",
    "    lab =[]\n",
    "    for k in range(len(N)):\n",
    "        if source:\n",
    "            \n",
    "            print(\"Loading data from source dataset: \", N[k])\n",
    "            \n",
    "        else:\n",
    "            print(\"Loading data from target dataset: \", N[k])  \n",
    "        for n in tqdm(range(len(dataname[N[k]]))):\n",
    "            if n==0:\n",
    "               path1 =os.path.join(root,datasetname[3], dataname[N[k]][n])\n",
    "            else:\n",
    "                path1 = os.path.join(root,datasetname[0], dataname[N[k]][n])\n",
    "            data1, lab1 = data_load(path1,dataname[N[k]][n],label=label_mapping[label[n]])\n",
    "            lab1=k*100+np.array(lab1)\n",
    "            lab1=lab1.tolist()\n",
    "            data += data1\n",
    "            lab +=lab1\n",
    "    return [data, lab]\n",
    "\n",
    "\n",
    "def data_load(filename, axisname, label):\n",
    "    '''\n",
    "    This function is mainly used to generate test data and training data.\n",
    "    filename:Data location\n",
    "    axisname:Select which channel's data,---->\"_DE_time\",\"_FE_time\",\"_BA_time\"\n",
    "    '''\n",
    "    datanumber = axisname.split(\".\")\n",
    "    if eval(datanumber[0]) < 100:\n",
    "        realaxis = \"X0\" + datanumber[0] + axis[0]\n",
    "    else:\n",
    "        realaxis = \"X\" + datanumber[0] + axis[0]\n",
    "    fl = loadmat(filename)[realaxis]\n",
    "    data = []\n",
    "    lab = []\n",
    "    start, end = 0, signal_size\n",
    "    while end <= fl.shape[0]:\n",
    "        data.append(fl[start:end])\n",
    "        lab.append(label)\n",
    "        start += signal_size\n",
    "        end += signal_size\n",
    "\n",
    "    return data, lab\n",
    "\n",
    "def balance_data(data_pd):\n",
    "    count = data_pd.value_counts(subset='label')\n",
    "    min_len = min(count) - 1\n",
    "    df = pd.DataFrame(columns=('data', 'label'))\n",
    "    for i in count.keys():\n",
    "        data_pd_tmp = data_pd[data_pd['label'] == i].reset_index(drop=True)\n",
    "        df = pd.concat([df, data_pd_tmp.loc[:min_len, ['data', 'label']]], ignore_index=True)\n",
    "    return df\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class multi_CWRU(object):\n",
    "    num_classes = 10\n",
    "    inputchannel = 1\n",
    "    def __init__(self, data_dir, transfer_task, normlizetype=\"0-1\"):\n",
    "        self.data_dir = data_dir+\"/CWRU\"\n",
    "        self.source_N = transfer_task[0]\n",
    "        print('self.source_N',self.source_N)\n",
    "        self.target_N = transfer_task[1]\n",
    "        self.normlizetype = normlizetype\n",
    "        self.data_transforms = {\n",
    "            'train': Compose([\n",
    "                Reshape(),\n",
    "                Normalize(self.normlizetype),\n",
    "                # RandomAddGaussian(),\n",
    "                # RandomScale(),\n",
    "                # RandomStretch(),\n",
    "                # RandomCrop(),\n",
    "                Retype(),\n",
    "                # Scale(1)\n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                Reshape(),\n",
    "                Normalize(self.normlizetype),\n",
    "                Retype(),\n",
    "                # Scale(1)\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def data_split(self, transfer_learning=True):\n",
    "        if transfer_learning:\n",
    "            # get source train and val\n",
    "            list_data = get_files(self.data_dir, self.source_N,source=True)\n",
    "            data_pd = pd.DataFrame({\"data\": list_data[0], \"label\": list_data[1]})\n",
    "            train_pd, val_pd = train_test_split(data_pd, test_size=0.2, random_state=40, stratify=data_pd[\"label\"])\n",
    "            source_train = dataset(list_data=train_pd, transform=self.data_transforms['train'])\n",
    "            source_val = dataset(list_data=val_pd, transform=self.data_transforms['val'])\n",
    "\n",
    "            # get target train and val\n",
    "            list_data = get_files(self.data_dir, self.target_N,source=False)\n",
    "            data_pd = pd.DataFrame({\"data\": list_data[0], \"label\": list_data[1]})\n",
    "            train_pd, val_pd = train_test_split(data_pd, test_size=0.2, random_state=40, stratify=data_pd[\"label\"])\n",
    "            target_train = dataset(list_data=train_pd, transform=self.data_transforms['train'])\n",
    "            target_val = dataset(list_data=val_pd, transform=self.data_transforms['val'])\n",
    "            \n",
    "            print(\"Source Train Dataset Size:\", len(source_train))\n",
    "            print(\"Source Validation Dataset Size:\", len(source_val))\n",
    "            print(\"Target Train Dataset Size:\", len(target_train))\n",
    "            print(\"Target Validation Dataset Size:\", len(target_val))\n",
    "\n",
    "            # 레이블에 따른 샘플 수 출력\n",
    "            print(\"Source Train Label Counts:\")\n",
    "            print(train_pd['label'].value_counts())\n",
    "            print(\"Source Validation Label Counts:\")\n",
    "            print(val_pd['label'].value_counts())\n",
    "            print(\"Target Train Label Counts:\")\n",
    "            print(train_pd['label'].value_counts())\n",
    "            print(\"Target Validation Label Counts:\")\n",
    "            print(val_pd['label'].value_counts())\n",
    "            \n",
    "            return source_train, source_val, target_train, target_val\n",
    "        else:\n",
    "            #get source train and val\n",
    "            list_data = get_files(self.data_dir, self.source_N)\n",
    "            data_pd = pd.DataFrame({\"data\": list_data[0], \"label\": list_data[1]})\n",
    "            train_pd, val_pd = train_test_split(data_pd, test_size=0.2, random_state=40, stratify=data_pd[\"label\"])\n",
    "            source_train = dataset(list_data=train_pd, transform=self.data_transforms['train'])\n",
    "            source_val = dataset(list_data=val_pd, transform=self.data_transforms['val'])\n",
    "\n",
    "            # get target train and val\n",
    "            list_data = get_files(self.data_dir, self.target_N)\n",
    "            data_pd = pd.DataFrame({\"data\": list_data[0], \"label\": list_data[1]})\n",
    "            target_val = dataset(list_data=data_pd, transform=self.data_transforms['val'])\n",
    "            return source_train, source_val, target_val\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    def data_split(self):\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.source_N [3, 1]\n",
      "Loading data from source dataset:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1253.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from source dataset:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1139.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from target dataset:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 974.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train Dataset Size: 2466\n",
      "Source Validation Dataset Size: 617\n",
      "Target Train Dataset Size: 1231\n",
      "Target Validation Dataset Size: 308\n",
      "Source Train Label Counts:\n",
      "0    378\n",
      "2    285\n",
      "3    284\n",
      "1    284\n",
      "Name: label, dtype: int64\n",
      "Source Validation Label Counts:\n",
      "0    95\n",
      "3    71\n",
      "2    71\n",
      "1    71\n",
      "Name: label, dtype: int64\n",
      "Target Train Label Counts:\n",
      "0    378\n",
      "2    285\n",
      "3    284\n",
      "1    284\n",
      "Name: label, dtype: int64\n",
      "Target Validation Label Counts:\n",
      "0    95\n",
      "3    71\n",
      "2    71\n",
      "1    71\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transfer_tasks = ([3,1], [2])  # Using domains 1 and 2 (1772 and 1750 RPM) as sources and domain 3 (1730 RPM) as target\n",
    "cwru_data = multi_CWRU(data_dir='/home/workspace/UDA_Bearing_Fault_Diagnosis/Data', transfer_task=transfer_tasks)\n",
    "\n",
    "# Split data for training (considering transfer learning)\n",
    "source_train, source_val, target_train, target_val = cwru_data.data_split(transfer_learning=True)#,imbalance_ratio={0:1,1: 0.5, 2: 0.3, 3: 0.1,4: 0.1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.SequenceDatasets.dataset at 0x7f08214db9e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([71, 71, 71, 71]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_val.labels\n",
    "np.unique(source_val.labels, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
